logs:
  path: /logs

dataset:
  path: /dataset/aclImdb

tokenizer:
  path: /imdb_bpe

training:
  batch_size: 64
  lr: 0.001
  num_epochs: 10
  scheduler: null     # YAML uses null, not None
  optimizer:
    name: adam
    beta1: 0.9
    beta2: 0.98
    epsilon: 1e-8

model:
  name: encoder
  embed_dim: 128
  max_len: 256
  vocab_size: 30000   # underscores can break PyYAML
  num_heads: 8
  num_layers: 4
  num_classes: 2
  attention: "multi-head"   # hyphen must be quoted

  # transformer specific hyperparameters
  d_model: 512
  num_encoder_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  activation: relu

  custom_encoder: null
  layer_norm_eps: 1e-5
  batch_first: false
  norm_first: false
  bias: true